Задача:

Сделать многопоточную сортировку 32-х разрядных целых чисел (по возрастанию или убыванию) из
двоичного файла, размер файла значительно больше чем доступная память.

Ожидаемый результат:

* C или С++ код собираемый на GCC 4.8 при помощи make (можно cmake)

* Проект должен собираться и запускаться без установки дополнительных библиотек."


Программа должна использовать заранее определенное количество выделяемой памяти,
которое можно задать ключами в командной строки при запуске.
Вы можете рассчитывать на то, что свободного места на диске в 2 раза больше,
ем размер сортируемого файла.
Максимальное количество потоков также можно задать из командной строки.


================================================================================
================================================================================


Описание алгоритма:

Из доступной памяти выделим буфер для записи в файл (8 МБ). Остальную память будем использовать для
загрузки и обработки данных из входных файлов.

Первый этап работы:
Из исходного файла будем загружать данные частями, насколько позволяет массив памяти.
Загруженные данные распределяем на N примерно равных частей, по числу доступных потоков исполнения.
Каждый поток сортирует свою часть с помощью std::sort().
Если загружено данных меньше, чем 16 МБ, то сортируем только одним потоком.

Затем, объединяем отсортированные части, чтобы в результате получился отсортированный массив.
Результат объединения записываем, через буфер записи, во временный файл.

Опять загружаем из исходного файла, с записью отсортированного результата во второй
временный файл, и т.д., пока не кончится исходный файл.

Второй этап:
У нас есть несколько временных файлов первой очереди, каждый с размером равным размеру памяти для обработки.
Каждый временный файл отсортирован, все вместе они содержат все исходные данные.

Будем объединять эти файлы по K штук. Путём загрузки в память, и слияния в один отсортированный
файл, содержащий данные всех К файлов. Число K сделаем константой, ее размер надо подобрать для
оптимальной работы (t_sorter::merge_files_max_k_).
Здесь многопоточную работу использовать не получается.
Можно в целях некоторой оптимизации, выполнять запись буфера в выходной файл в отдельном потоке.
А основной поток в это время выполняет объединение данных в другой буфер. (Не реализовано в первой версии).
(Можно было вместо этого попробовать использовать асинхронные файловые операции).

Если на предыдущем этапе получилось больше K файлов, то они будут слиты
по K штук в разные отсортированные файлы. Это будут временные файлы второй очереди. Их размер
примерно в K раз больше файлов первой очереди. Временные файлы первой очереди теперь можно удалить.
Далее при необходимости повторяем процедуру укрупнения файлов.
Когда получился только 1 выходной временный файл - то он будет результатом сортировки исходного файла.


TODO:
1. При слиянии данных, сделать запись в файл в отдельном потоке, как и планировалось.
   Сейчас все операции при слиянии делаются в одном потоке.

2. Понять, как влияет t_sorter::merge_files_max_k_ на скорость работы.


================================================================================
================================================================================

Сборка:

cd src
make

Запуск:

./sorter1 input_file 1100 4

Выделяется 1100 МБ памяти (не считая небольшого количества памяти для объектов).
Запускается 4 потока (загружающих процессор), для сортировки, по возрастанию. Ещё один поток будет главным,
ждать результата сортировки частей.
При слиянии данных, в любом случае, один поток выполняет слияние, другой вызывает файловые операции
для записи. (Не реализовано в первой версии).
Временные файлы создаются в текущей директории.
Файл с результатом сортировки: sort_result.bin

./sorter1 input_file -desc 1100 4

Сортирует по убыванию.

Порядок байтов в 32-битных числах, используется соответствующий архитектуре процессора.

Минимум памяти: 32 МБ (можно уменьшить в константах t_sorter).
Задаваемое число потоков ограничено 1..32.



